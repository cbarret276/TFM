from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.sensors.external_task_sensor import ExternalTaskSensor
from airflow.utils.dates import days_ago
from datetime import datetime, timedelta
from elasticsearch import Elasticsearch, helpers
import ast

# Elasticsearch Index Names
HOURLY_INDEX_TEMPLATE = "gold_mw_agr_hourly_*"  # Hourly aggregated indices
GOLD_INDEX_TEMPLATE = "gold_mw_agr_daily_{date}"  # Template for daily Gold index names

# Default Arguments
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2025, 5, 30), #days_ago(6)
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=10),
}

# DAG Definition
dag = DAG(
    'gen_gold_mw_daily_index',
    default_args=default_args,
    description='DAG to process data from Bronze index to Gold agregate daily index in Elasticsearch',
    schedule_interval='@daily',  # Run daily at midnight
    catchup=True
)

# Elasticsearch Connection Setup
def get_elasticsearch_client():
    """Creates and returns an Elasticsearch client."""
    return Elasticsearch(
        ["http://elasticsearch:9200"],  # Elasticsearch host URL
        request_timeout=60,  # Request timeout in seconds
        max_retries=5,  # Number of retries on failure
        retry_on_timeout=True  # Retry if timeout occurs
    )

# Task 1: External Task Sensor to ensure hourly DAG completion
wait_for_hourly_dag = ExternalTaskSensor(
    task_id="wait_for_hourly_dag",
    external_dag_id="gen_gold_malw_hourly_index",  # DAG ID of the hourly DAG
    external_task_id="create_and_insert_into_gold_index",  # Task ID of the last task in Bronze DAG
    allowed_states=["success"],  # Only proceed if the task has completed successfully
    failed_states=["failed", "skipped"],  # Fail if the Bronze task failed
    execution_date_fn=lambda dt: dt + timedelta(hours=23),  # Last hour of the previous day
    mode="reschedule",  # Free up worker slots while waiting
    timeout=365 * 31 * 24 * 60 * 60,  # No timeout for this sensor
    poke_interval=3600,  # Intervalo entre verificaciones (en segundos)
    dag=dag,
)

# Task 2: Fetch Aggregated Data from  Hourly Indices
def fetch_aggregated_data(**context):
    """Fetches aggregated data from the Bronze index for the given time window."""
    
    # Time Window
    start_date = context['data_interval_start'].replace(microsecond=0).isoformat().replace('+00:00', 'Z')
    end_date = context['data_interval_end'].replace(microsecond=0).isoformat().replace('+00:00', 'Z') 
    
    print("Date range: ",f"from:{start_date} to:{end_date}")

    es = get_elasticsearch_client()

    # Query Aggregations from Hourly Indices
    query = {
        "query": {
            "range": {
                "date": {  # Filter by date in hourly indices
                    "gte": start_date,
                    "lte": end_date,
                    "format": "strict_date_optional_time"
                }
            }
        },
        "aggs": {
            "by_family": {
                "terms": {"field": "family", "size": 300, "missing": "Unknown"},
                "aggs": {
                    "avg_score": {"avg": {"field": "avg_score"}},
                    "total_count": {"sum": {"field": "count"}}
                }
            },
            "unique_domains": {"sum": {"field": "unique_domains"}},  # Sum up unique counts
            "unique_ips": {"sum": {"field": "unique_ips"}}
        },
        "size": 0  # Only aggregations
    }

    results = es.search(index=HOURLY_INDEX_TEMPLATE, body=query)

    # Extract aggregated data for Gold Index
    family_data = [
        {"family": bucket["key"], 
         "avg_score": bucket["avg_score"]["value"], 
         "count": bucket["total_count"]["value"]}
        for bucket in results["aggregations"]["by_family"]["buckets"]
    ]

    unique_counts = {
        "unique_domains": results["aggregations"]["unique_domains"]["value"],
        "unique_ips": results["aggregations"]["unique_ips"]["value"]
    }

    es.close()

    return {
        "family_data": family_data,
        "unique_counts": unique_counts
    }

# Task 3: Insert Aggregated Data into Gold Index
def insert_into_gold_index(aggregated_data, **context):
    """Inserts the aggregated data into the Gold index."""
    es = get_elasticsearch_client()
    
    execution_date = context['data_interval_start']
    gold_index = GOLD_INDEX_TEMPLATE.format(date=execution_date.strftime("%Y%m"))

    # Define mapping
    mapping = {
        "mappings": {
            "properties": {
                "type": {"type": "keyword"},
                "date": {"type": "date"},
                "family": {"type": "keyword"},
                "avg_score": {"type": "float"},
                "count": {"type": "integer"},
                "unique_domains": {"type": "long"},
                "unique_ips": {"type": "long"}
            }
        }
    }

    # Create index with mapping
    if not es.indices.exists(index=gold_index):
        es.indices.create(index=gold_index, body=mapping)
        print(f"Index {gold_index} created with mapping.")

    # Prepare Data for Bulk Insert
    actions = []
    aggregated_data = ast.literal_eval(aggregated_data)

    for record in aggregated_data["family_data"]:
        actions.append({
            "_op_type": "index",
            "_index": gold_index,
            "_source": {
                "type": "family",
                "date": execution_date.strftime("%Y-%m-%d"),
                "family": record["family"],
                "avg_score": record["avg_score"],
                "count": record["count"]
            }
        })

    actions.append({
        "_op_type": "index",
        "_index": gold_index,
        "_source": {
            "type": "summary",
            "date": execution_date.strftime("%Y-%m-%d"),
            "unique_domains": aggregated_data["unique_counts"]["unique_domains"],
            "unique_ips": aggregated_data["unique_counts"]["unique_ips"]
        }
    })

    # Perform bulk insert with error control
    if actions:
        try:
            print(actions)
            helpers.bulk(es, actions)
            print(f"Gold index {gold_index} created successfully with {len(actions)} records.")    
        except helpers.BulkIndexError as e:
            print(f"Bulk update error: {e.errors}")
            raise
    else:
        print(f"No data to insert into {gold_index}.")

    es.close()

# Define Tasks in the DAG
fetch_aggregated_data_task = PythonOperator(
    task_id='fetch_aggregated_data',
    python_callable=fetch_aggregated_data,
    provide_context=True,
    execution_timeout=timedelta(minutes=30),
    dag=dag,
)

insert_into_gold_index_task = PythonOperator(
    task_id='insert_into_gold_index',
    python_callable=insert_into_gold_index,
    provide_context=True,
    op_args=['{{ ti.xcom_pull(task_ids="fetch_aggregated_data") }}'],
    execution_timeout=timedelta(minutes=30),
    dag=dag,
)

# Define Task Dependencies
wait_for_hourly_dag >> fetch_aggregated_data_task >> insert_into_gold_index_task
