from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.sensors.external_task_sensor import ExternalTaskSensor
from airflow.utils.dates import days_ago
from datetime import datetime, timedelta
from elasticsearch import Elasticsearch, helpers
import ast

# Elasticsearch Index Names
BRONZE_INDEX = "bronze_mw_raw"
GOLD_INDEX_TEMPLATE = "gold_mw_agr_hourly_{date}"  # Template for hourly Gold index names

# Default Arguments
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2025, 5, 26, 4, 5), #days_ago(6)
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

# DAG Definition
dag = DAG(
    'gen_gold_malw_hourly_index',
    default_args=default_args,
    description='DAG to process data from Bronze index to Gold aggregated hourly index in Elasticsearch',
    schedule_interval='@hourly',  # Run hourly
    catchup=True,
    max_active_runs=2,  # Allow only two running intances at the same time
)

# Elasticsearch Connection Setup
def get_elasticsearch_client():
    """Creates and returns an Elasticsearch client."""
    return Elasticsearch(
        ["http://elasticsearch:9200"],
        request_timeout=60,
        max_retries=5,
        retry_on_timeout=True
    )

# Task 1: External Task Sensor to ensure Bronze DAG completion
wait_for_bronze_dag = ExternalTaskSensor(
    task_id="wait_for_bronze_dag",
    external_dag_id="gen_bronze_mw_raw_index",
    external_task_id="fetch_sample_details",
    allowed_states=["success"],
    failed_states=["failed", "skipped"],
    execution_date_fn=lambda dt: dt + timedelta(hours=2),  # Wait 1 hour
    mode="reschedule",
    timeout=31 * 365 * 24 * 60 * 60,   # Don`t expire the task
    poke_interval=300, # Time interval to check the sensor state
    dag=dag,
)

# Task 2: Fetch Aggregated Data from Bronze Index
def fetch_aggregated_data(**context):
    """Fetches aggregated data from the Bronze index for the given time window."""
    
    start_date = context['data_interval_start'].replace(microsecond=0).isoformat().replace('+00:00', 'Z')
    end_date = context['data_interval_end'].replace(microsecond=0).isoformat().replace('+00:00', 'Z')

    print("Date range: ",f"from:{start_date} to:{end_date}")

    es = get_elasticsearch_client()

    query = {
        "query": {
            "range": {
                "created": {
                    "gte": start_date,
                    "lte": end_date,
                    "format": "strict_date_optional_time"
                }
            }
        },
        "aggs": {
            "by_family": {
                "terms": {"field": "family", "size": 300, "missing": "Unknown"},
                "aggs": {
                    "avg_score": {"avg": {"field": "score"}},
                    "total_count": {"value_count": {"field": "score"}}
                }
            },
            "unique_domains": {"cardinality": {"field": "domains"}},
            "unique_ips": {"cardinality": {"field": "ips"}}
        },
        "size": 0
    }

    results = es.search(index=BRONZE_INDEX, body=query)

    print("Num of docs:", results["hits"]["total"]["value"])

    family_data = [
        {"family": bucket["key"], 
         "avg_score": bucket["avg_score"]["value"], 
         "count": bucket["total_count"]["value"]}
        for bucket in results["aggregations"]["by_family"]["buckets"]
    ]

    unique_counts = {
        "unique_domains": results["aggregations"]["unique_domains"]["value"],
        "unique_ips": results["aggregations"]["unique_ips"]["value"]
    }

    es.close()

    return {
        "family_data": family_data,
        "unique_counts": unique_counts
    }

# Task 3: Create Index with Optimized Mapping and Insert Data
def create_and_insert_into_gold_index(aggregated_data, **context):
    """Creates a Gold index with an optimized mapping and inserts the aggregated data."""
    es = get_elasticsearch_client()

    execution_date = context['data_interval_start']
    gold_index = GOLD_INDEX_TEMPLATE.format(date=execution_date.strftime("%Y%m%d"))

    # Define mapping
    mapping = {
        "mappings": {
            "properties": {
                "type": {"type": "keyword"},
                "date": {"type": "date"},
                "family": {"type": "keyword"},
                "avg_score": {"type": "float"},
                "count": {"type": "integer"},
                "unique_domains": {"type": "long"},
                "unique_ips": {"type": "long"}
            }
        }
    }

    # Create index with mapping
    if not es.indices.exists(index=gold_index):
        es.indices.create(index=gold_index, body=mapping)
        print(f"Index {gold_index} created with mapping.")

    # Prepare data for bulk insert
    actions = []
    aggregated_data = ast.literal_eval(aggregated_data)

    for record in aggregated_data["family_data"]:
        actions.append({
            "_op_type": "index",
            "_index": gold_index,
            "_source": {
                "type": "family",
                "date": execution_date.strftime("%Y-%m-%dT%H:00:00"),
                "family": record["family"],
                "avg_score": record["avg_score"],
                "count": record["count"]
            }
        })

    actions.append({
        "_op_type": "index",
        "_index": gold_index,
        "_source": {
            "type": "summary",
            "date": execution_date.strftime("%Y-%m-%dT%H:00:00"),
            "unique_domains": aggregated_data["unique_counts"]["unique_domains"],
            "unique_ips": aggregated_data["unique_counts"]["unique_ips"]
        }
    })

    if actions:
        try:
            helpers.bulk(es, actions)
            print(f"Data successfully inserted into {gold_index}.")
        except helpers.BulkIndexError as e:
            print(f"Error during bulk insert: {e.errors}")

    es.close()

# Define Tasks in the DAG
fetch_aggregated_data_task = PythonOperator(
    task_id='fetch_aggregated_data',
    python_callable=fetch_aggregated_data,
    provide_context=True,
    dag=dag,
)

create_and_insert_task = PythonOperator(
    task_id='create_and_insert_into_gold_index',
    python_callable=create_and_insert_into_gold_index,
    provide_context=True,
    op_args=['{{ ti.xcom_pull(task_ids="fetch_aggregated_data") }}'],
    dag=dag,
)

# Define Task Dependencies
wait_for_bronze_dag >> fetch_aggregated_data_task >> create_and_insert_task
